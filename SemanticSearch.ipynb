{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yagth/SemanticSearch/blob/main/SemanticSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXvw7HgEBUmQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip uninstall transformers\n",
        "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "!pip -q install git+https://github.com/huggingface/peft.git\n",
        "!pip -q install bitsandbytes\n",
        "!pip install --upgrade langchain  -q\n",
        "!pip install pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ERdW8zSedFxD"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/lm-sys/FastChat.git\n",
        "%cd FastChat"
      ],
      "metadata": {
        "id": "toRTfKdLO9kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade pip  # enable PEP 660 support\n",
        "!pip3 install -e ."
      ],
      "metadata": {
        "id": "OeBH128lO9im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gxSbuLRY6NpP"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig\n",
        "import textwrap\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import pinecone\n",
        "\n",
        "model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOWzbTZlTw5M"
      },
      "source": [
        "# Functions to scarp web data\n",
        "Scap web data provided link from the user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GUmc3uh6jP9"
      },
      "outputs": [],
      "source": [
        "def get_html_content(url):\n",
        "    response = requests.get(url)\n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao498Caz6mHG"
      },
      "outputs": [],
      "source": [
        "def get_plain_text(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    for script in soup([\"script\"]):\n",
        "        script.extract()\n",
        "    return soup.get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7-yjzAI6rkC"
      },
      "outputs": [],
      "source": [
        "def split_text_into_chunks(plain_text, max_chars=2000):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = max_chars,           # Usually chunk sizes are much larger than this\n",
        "      chunk_overlap  = 20,        # Overlap is needed incase the text is split in odd places\n",
        "      length_function = len,\n",
        "    )\n",
        "    chunks = text_splitter.split_text(plain_text)\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CoOj5LCPbQl"
      },
      "outputs": [],
      "source": [
        "def scrape_text_from_url(url, max_chars=2000):\n",
        "    html_content = get_html_content(url)\n",
        "    plain_text = get_plain_text(html_content)\n",
        "    text_chunks = split_text_into_chunks(plain_text, max_chars)\n",
        "    return text_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run this if you want to scrap web instead of load json"
      ],
      "metadata": {
        "id": "3UaHTLH8uQSk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_nQbQuX6y4p"
      },
      "outputs": [],
      "source": [
        "url = input(\"Enter the URL to scrape text from: \")\n",
        "plain_text_chunks = scrape_text_from_url(url)\n",
        "print(plain_text_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions to clean and extract data from a json file.\n"
      ],
      "metadata": {
        "id": "19r-t5ITP525"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing gitpython and cloning the repo. that contains the exported data from slack"
      ],
      "metadata": {
        "id": "SvoirdZnfr91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gitpython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9nyelUwfxAL",
        "outputId": "069f9d8f-f2f8-48f0-d3d5-634cdcdb3e82"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/184.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.10 gitpython-3.1.31 smmap-5.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import git\n",
        "import os\n",
        "import json\n",
        "import datetime as dt"
      ],
      "metadata": {
        "id": "1VECBfnVocan"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_url = \"https://github.com/Yagth/MatterMost-LLM-test-Slack-export-Jun-19-2023---Jun-20-2023.git\"\n",
        "local_path = \"/content/slackdata\"\n",
        "\n",
        "git.Repo.clone_from(repo_url, local_path)"
      ],
      "metadata": {
        "id": "kxFfa-Rtf4tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8b034db-504c-49ba-ffbb-7a9d79e35e1e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<git.repo.base.Repo '/content/slackdata/.git'>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pull updates from a repo.\n",
        "Run this if you alread have clone the repo."
      ],
      "metadata": {
        "id": "6BXgJM5FpVTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xu0ZSd10pVQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo = git.Repo('/content/slackdata/')\n",
        "repo.git.pull()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "K3AJ1uOapXrn",
        "outputId": "84221bd1-5c79-43de-adb5-357f4229e940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Updating 1e83421..84b977f\\nFast-forward\\n gptgenerated/{2023-06-30.json => 2023-06-19.json} | 0\\n 1 file changed, 0 insertions(+), 0 deletions(-)\\n rename gptgenerated/{2023-06-30.json => 2023-06-19.json} (100%)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions that are used to load the json data of messages from the channels"
      ],
      "metadata": {
        "id": "Qr-ciUONgZri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadMetadata(filePath):\n",
        "  with open(filePath, 'r') as file:\n",
        "    jsonString = file.read()\n",
        "\n",
        "  messages = json.loads(jsonString)\n",
        "  metadatas = []\n",
        "\n",
        "  for message in messages:\n",
        "    timestamp = float(message['ts'])\n",
        "    date = dt.datetime.fromtimestamp(timestamp)\n",
        "    date = date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    #If just to filter out channel joined messages which are irrelevant.\n",
        "    if not ('subtype' in message):\n",
        "      metadata = {\n",
        "        'name':message['user_profile']['real_name'],\n",
        "        'message':message['text'],\n",
        "        'time': date,\n",
        "        'channel':'gptgenerated',\n",
        "        'ispublic':True\n",
        "      }\n",
        "      metadatas.append(metadata)\n",
        "\n",
        "  return metadatas\n",
        "\n",
        "def extractMessage(metadatas):\n",
        "  messages = []\n",
        "  for metadata in metadatas:\n",
        "    messages.append(metadata['message'])\n",
        "\n",
        "  return messages"
      ],
      "metadata": {
        "id": "N-YV_Cy4ggd0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadatas = loadMetadata('/content/slackdata/gptgenerated/2023-06-19.json')\n",
        "messages = extractMessage(metadatas)"
      ],
      "metadata": {
        "id": "VGXiCvFntGQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63436012-43fc-40b4-e783-297dbecabb11"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': 'John Smith', 'message': \"Do you know any alternative LLM models that can match GPT3's capabilities?\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Alice Johnson', 'message': 'I heard OpenAI has released GPT4, which is supposed to be an improvement over GPT3.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Sarah Davis', 'message': \"I've been using GPT2 for similar tasks, and it has been working well for me.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Michael Johnson', 'message': \"I'm not familiar with LLM models, but I can look into it and provide some suggestions.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'John Smith', 'message': 'GPT3 has been the go-to choice for most applications, but exploring alternatives is always a good idea.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Alice Johnson', 'message': \"I think Microsoft's Turing model could be a potential alternative to GPT3. It's worth checking out.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Sarah Davis', 'message': 'GPT3 has set the bar high, but we should keep an eye on research papers and see if any promising alternatives are being developed.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Michael Johnson', 'message': \"I believe Google's Reformer model could be a potential alternative to GPT3. It's designed to handle long-range dependencies efficiently.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'John Smith', 'message': \"Has anyone tried EleutherAI's GPT-Neo? It's an open-source alternative to GPT3 and might be worth exploring.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Alice Johnson', 'message': 'BERT is another popular model that could be considered as an alternative to GPT3. It has been widely used for various NLP tasks.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Sarah Davis', 'message': 'I heard OpenAI is working on GPT4, which might offer significant improvements over GPT3. We should keep an eye on their advancements.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Michael Johnson', 'message': \"What about XLNet? It's a transformer-based model that has shown promising results in various NLP benchmarks.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'John Smith', 'message': \"We could consider using GPT-2 as an alternative to GPT3. Although it's an older model, it still performs well in many applications.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Alice Johnson', 'message': \"Have you looked into CTRL? It's a conditional transformer language model that allows fine-grained control over the generated text.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Sarah Davis', 'message': \"Transformer-XL is another option worth considering. It's designed to handle longer sequences and has achieved impressive results.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Michael Johnson', 'message': \"You might want to explore T5 as an alternative to GPT3. It's a versatile model that has achieved state-of-the-art performance on various NLP tasks.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'John Smith', 'message': \"One potential alternative to GPT3 is UniLM. It's a unified language model that has achieved competitive performance across various NLP tasks.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Alice Johnson', 'message': \"If you're looking for a more lightweight alternative, you can consider DistilBERT. It's a distilled version of BERT that offers faster inference times.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Sarah Davis', 'message': \"BERT is a widely-used model that can serve as a good alternative to GPT3. It's particularly effective for tasks involving natural language understanding.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Michael Johnson', 'message': \"You may want to explore RoBERTa as an alternative to GPT3. It's a robustly optimized BERT model that has achieved state-of-the-art results on several benchmarks.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'John Smith', 'message': 'I think GPT3 has a wide range of applications and generates highly coherent text, but it can be quite expensive and may not be necessary for our semantic search application.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Alice Johnson', 'message': \"You're right, GPT3 might be overkill for our needs. CTRL seems promising as it offers fine-grained control over the generated text, which can be valuable for semantic search.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Sarah Davis', 'message': 'T5 is known for its versatility and state-of-the-art performance across various NLP tasks. It could be a suitable choice for semantic search in our application.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Michael Johnson', 'message': 'UniLM is another alternative worth considering. Its competitive performance across various NLP tasks makes it a strong candidate for semantic search in our application.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'John Smith', 'message': 'DistilBERT could be a good fit for our semantic search application, thanks to its lightweight nature and fast inference times.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Alice Johnson', 'message': 'BERT offers strong performance in natural language understanding tasks and can be a reliable choice for semantic search in our application.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Sarah Davis', 'message': 'RoBERTa has achieved state-of-the-art results on various benchmarks and can provide high-quality semantic search capabilities for our application.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Michael Johnson', 'message': 'XLNet is known for its ability to capture bidirectional contexts effectively, which can be beneficial for semantic search in our application.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'John Smith', 'message': 'ERNIE has shown promising results in understanding natural language semantics, making it a potential choice for semantic search in our application.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Alice Johnson', 'message': 'ALBERT, with its parameter-efficient design, can provide competitive performance in semantic search while requiring less computational resources.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Sarah Davis', 'message': 'Electra, with its pre-training approach based on adversarial training, can provide effective semantic search capabilities for our application.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'John Smith', 'message': 'I think BERT would be the ideal choice for our semantic search application. It has proven performance and is widely used in the industry.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Alice Johnson', 'message': 'I agree with John. BERT has a strong track record and has been extensively tested in various applications.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Michael Johnson', 'message': \"I understand your point, but GPT3 may not be the most suitable model for semantic search. BERT's fine-tuning capabilities make it a better fit for our specific task.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Sarah Davis', 'message': \"I see your point now. BERT's fine-tuning capabilities can help us achieve better accuracy and efficiency in our semantic search application.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'John Smith', 'message': 'GPT3 is a powerful model, no doubt, but its size and computational requirements make it less practical for our semantic search application.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Alice Johnson', 'message': \"Absolutely. BERT's smaller size allows for faster training and deployment, which is crucial for our application's performance.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Michael Johnson', 'message': \"You're right. BERT's fine-tuning process enables us to adapt the model specifically to our semantic search task and improve its performance.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Sarah Davis', 'message': \"BERT's pre-training on large-scale data also helps it capture better contextual understanding, which is crucial for semantic search.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'John Smith', 'message': \"GPT3's generative nature may introduce unnecessary complexity in our semantic search application. BERT's focus on understanding context is more aligned with our goals.\", 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}, {'name': 'Michael Johnson', 'message': 'I agree with you all now. BERT seems to be the better choice for our semantic search application, considering its practicality and performance.', 'time': '2023-06-19', 'channel': 'gptgenerated', 'ispublic': True}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t9Xn3AzULx_"
      },
      "source": [
        "# Vector Database setup\n",
        "set up pinecone vector database and function to insert data along with a method to insert the datas in chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "viHnRkPcSzeP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa12f98-995d-42ad-c119-09c4d633fa2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 384,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'': {'vector_count': 41}},\n",
              " 'total_vector_count': 41}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "pinecone.init(api_key=\"API_KEY\", environment=\"ENV\")\n",
        "index = pinecone.Index(\"INDEX-NAME\")\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yShks1P8S10Z"
      },
      "outputs": [],
      "source": [
        "def addData(corpusData,url):\n",
        "    id  = index.describe_index_stats()['total_vector_count']\n",
        "    step = min(100, len(corpusData)) #The number of vectors to insert into the database at the same time\n",
        "    records = []\n",
        "    for i in range(len(corpusData)):\n",
        "        chunk=corpusData[i]\n",
        "        chunkInfo=(str(id+i),\n",
        "                model.embed_query(chunk),\n",
        "                {'title': url,'context': chunk})\n",
        "        records.append(chunkInfo)\n",
        "        if len(records) >= step:\n",
        "          index.upsert(vectors=records)\n",
        "          print('Batch no. ' + str(len(records)))\n",
        "          records = []\n",
        "\n",
        "def addMessage(messages, metadatas):\n",
        "    id  = index.describe_index_stats()['total_vector_count']\n",
        "    step = min(100, len(messages)) #The number of vectors to insert into the database at the same time\n",
        "    records = []\n",
        "    for i in range(len(messages)):\n",
        "        chunk= messages[i]\n",
        "        chunkInfo=(str(id+i),\n",
        "                model.embed_query(chunk),\n",
        "                metadatas[i])\n",
        "        records.append(chunkInfo)\n",
        "\n",
        "        if len(records) >= step:\n",
        "          index.upsert(vectors=records)\n",
        "          print('Batch no. ' + str(len(records)))\n",
        "          records = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the following if you are scrapping web"
      ],
      "metadata": {
        "id": "pBh25zsRumL9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ff-7Jb-H7bW"
      },
      "outputs": [],
      "source": [
        "len(plain_text_chunks)\n",
        "plain_text_chunks[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx10Okg1fH0b"
      },
      "outputs": [],
      "source": [
        "addData(plain_text_chunks,url)\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the following if you are using json file"
      ],
      "metadata": {
        "id": "uZPPqbQcurf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "addMessage(messages, metadatas)\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBc0dRuuuup0",
        "outputId": "268d1097-d717-443d-fec8-dd7272c801c7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch no. 41\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 384,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'': {'vector_count': 41}},\n",
              " 'total_vector_count': 41}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B51s4xGkggXQ"
      },
      "outputs": [],
      "source": [
        "def find_match(query,k):\n",
        "    query_em = model.embed_query(query)\n",
        "    result = index.query(\n",
        "      query_em,\n",
        "      top_k=k,\n",
        "      include_values=True,\n",
        "      include_metadata=True\n",
        "    )\n",
        "\n",
        "    return [result['matches'][i]['metadata'] for i in range(k)],[result['matches'][i]['metadata']['message'] for i in range(k)],[result['matches'][i]['score'] for i in range(k)]\n",
        "\n",
        "def printResult(datas, metadatas, scores):\n",
        "  for i in range(len(datas)):\n",
        "    print(\"Entry: \" + str(i))\n",
        "    print(\"\\tData: \" + datas[i])\n",
        "    print(\"\\tScore: \" + str(scores[i]))\n",
        "    print(\"\\tLink: \" + str(metadatas[i]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKikjxJ17-wK"
      },
      "source": [
        "#Prompt\n",
        "Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text and requires some latest information to be updated, print 'Sorry Not Sufficient context to answer query' \\n\"\n",
        "    return header + context + \"\\n\\n\" + query + \"\\n\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "i7uTwoD0WCnx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b0TJSLqthbtz"
      },
      "outputs": [],
      "source": [
        "def create_prompt(context,query):\n",
        "    # header = \"Answer the question as truthfully as possible using only the provided context, and if the answer is not contained within the text and requires some latest information to be updated, print 'Sorry Not Sufficient context to answer query' \\n\"\n",
        "    prompt = '''\n",
        "Give the most relevant response using only the context provided below.\n",
        "Don't use any other context outside the given context. If the given context isn't enough to give a response, print 'The given context isn't enough to give a valid respone'\n",
        "\n",
        "###Context:\n",
        "    {context}\n",
        "###Question:\n",
        "    > {query}\n",
        "###Response:\n",
        "    '''.format(context=context, query=query)\n",
        "    # return header +\"\\nContext:\" + context + \"\\n\\nQuestion: \" + query + \"\\n\"\n",
        "    return prompt\n",
        "\n",
        "def init_model():\n",
        "      # Load the model\n",
        "    print(\"Tokenizing model...\")\n",
        "\n",
        "\n",
        "    tokenizer = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
        "    # tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.3\")\n",
        "\n",
        "    print(\"Creating model...\")\n",
        "    model = LLaMAForCausalLM.from_pretrained(\n",
        "        \"decapoda-research/llama-7b-hf\",\n",
        "        load_in_8bit=True,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    print(\"Fine tunning model...\")\n",
        "    model = PeftModel.from_pretrained(model, \"samwit/alpaca7B-lora\")\n",
        "    # model = AutoModelForCausalLM.from_pretrained(model, \"lmsys/vicuna-7b-v1.3\")\n",
        "    # inputs = tokenizer(\n",
        "        # prompt,\n",
        "        # return_tensors=\"pt\",\n",
        "    # )\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "def generate_answer(prompt, tokenizer, model):\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "\n",
        "    print(\"Generating config model...\")\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2,\n",
        "    )\n",
        "    print(\"Generating...\")\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256,\n",
        "    )\n",
        "\n",
        "    # for s in generation_output.sequences:\n",
        "        # print(tokenizer.decode(s))\n",
        "    return (tokenizer.decode(generation_output.sequences[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0Q1m8a7qrcxl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "c2197a075e664905b8bd007de73dde47",
            "4984914dcb504be7b1ddcd9f313b03c7",
            "019dd42f63ff43668f74e716495c9c93",
            "e983281168dd4990a4c4fcd1d4cca79c",
            "e956e4385b7b4b7790e9069c267579aa",
            "ba4f2a7caf1a47b68c75bb22291b2c43",
            "21d1cfa65808472291f0cf1a00b3dace",
            "7da55f0eca024550bb9cb57c95524959",
            "82ab267003084f52933c7df21e6a5542",
            "0e1e61da7bdd46f4b3121f79121a72c7",
            "4b8001d09a494c8db3ee7ec9c0299b25"
          ]
        },
        "outputId": "dfb7e3be-d74a-41ef-886a-80117942a389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing model...\n",
            "Creating model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2197a075e664905b8bd007de73dde47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine tunning model...\n"
          ]
        }
      ],
      "source": [
        "tokenizer, modelNew = init_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3H-OHM4g47o"
      },
      "outputs": [],
      "source": [
        "question = \"What is decided about LLM Alternative?\"\n",
        "metadatas, datas, scores = find_match(question,5)\n",
        "printResult(datas, metadatas, scores)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combinedData = [[data, metadata] for data, metadata, score in zip(datas, metadatas, scores) if score > 0.2 and metadata['ispublic']]\n",
        "context = \"\\n\".join(\"\"+data[0] + \"\\nBy-> \"+data[1]['name'] +\"  date-> \"+str(data[1]['time'])+\"\\n\" for data in combinedData)\n",
        "if context == \"\":\n",
        "  context = \"Not enough context\"\n",
        "print(context)\n"
      ],
      "metadata": {
        "id": "aj4ytQbeX7M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXCBNxRIiZQP"
      },
      "outputs": [],
      "source": [
        "prompt = create_prompt(context, question)\n",
        "response = generate_answer(prompt, tokenizer, modelNew)\n",
        "lines = response.split(\"\\n\")\n",
        "response = lines[len(lines) - 1].strip()\n",
        "print(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web appliation server for the model\n",
        "This section involves creating a web server application using falsk that will host it externally using ngrok to be accessed globally."
      ],
      "metadata": {
        "id": "fRjAZjyJzhbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-ngrok"
      ],
      "metadata": {
        "id": "mXh91sWIzu_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing ngrok"
      ],
      "metadata": {
        "id": "gWTeI97Y7yOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "2L0RhBGk7WOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell is used to set up the ngrok with the environment variable need to run it. The xxxx is a place holder for the authentication token that can be found from the ngrok website"
      ],
      "metadata": {
        "id": "cMISplbF7h7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken xxxx"
      ],
      "metadata": {
        "id": "Fe7cQ2Fo7ZZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, render_template, jsonify\n",
        "from flask_ngrok import run_with_ngrok"
      ],
      "metadata": {
        "id": "fha1EYA60MPb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/generateResponse',methods=['POST'])\n",
        "def generateResponse():\n",
        "    question = request.form['searchStr']\n",
        "\n",
        "    metadatas, datas, scores = find_match(question,5)\n",
        "    printResult(datas, metadatas, scores)\n",
        "\n",
        "    combinedData = [[data, metadata] for data, metadata, score in zip(datas, metadatas, scores) if score > 0.2 and metadata['ispublic']]\n",
        "    context = \"\\n\".join(\"\"+data[0] + \"\\nBy-> \"+data[1]['name'] +\"  date-> \"+str(data[1]['time'])+\"\\n\" for data in combinedData)\n",
        "    if context == \"\":\n",
        "      context = \"Not enough context\"\n",
        "\n",
        "    prompt = create_prompt(context, question)\n",
        "    response = generate_answer(prompt, tokenizer, modelNew)\n",
        "    lines = response.split(\"\\n\")\n",
        "    response = lines[len(lines) - 1].strip()\n",
        "\n",
        "    data = {\n",
        "        \"response\": response,\n",
        "        \"context\": context,\n",
        "    }\n",
        "\n",
        "    return jsonify(data)\n",
        "\n",
        "app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCLlVDAB0cFo",
        "outputId": "6d18173f-1ced-48e2-9c67-dfd0d1425aef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://a218-34-82-30-163.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [01/Jul/2023 11:35:07] \"GET / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entry: 0\n",
            "\tData: I'm not familiar with LLM models, but I can look into it and provide some suggestions.\n",
            "\tScore: 0.725169599\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"I'm not familiar with LLM models, but I can look into it and provide some suggestions.\", 'name': 'Michael Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 1\n",
            "\tData: Do you know any alternative LLM models that can match GPT3's capabilities?\n",
            "\tScore: 0.503625393\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"Do you know any alternative LLM models that can match GPT3's capabilities?\", 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 2\n",
            "\tData: GPT3 is a powerful model, no doubt, but its size and computational requirements make it less practical for our semantic search application.\n",
            "\tScore: 0.111373752\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'GPT3 is a powerful model, no doubt, but its size and computational requirements make it less practical for our semantic search application.', 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 3\n",
            "\tData: UniLM is another alternative worth considering. Its competitive performance across various NLP tasks makes it a strong candidate for semantic search in our application.\n",
            "\tScore: 0.104968593\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'UniLM is another alternative worth considering. Its competitive performance across various NLP tasks makes it a strong candidate for semantic search in our application.', 'name': 'Michael Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 4\n",
            "\tData: You're right. BERT's fine-tuning process enables us to adapt the model specifically to our semantic search task and improve its performance.\n",
            "\tScore: 0.103794441\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"You're right. BERT's fine-tuning process enables us to adapt the model specifically to our semantic search task and improve its performance.\", 'name': 'Michael Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Generating config model...\n",
            "Generating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [01/Jul/2023 11:35:26] \"POST /generateResponse HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entry: 0\n",
            "\tData: I'm not familiar with LLM models, but I can look into it and provide some suggestions.\n",
            "\tScore: 0.725169599\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"I'm not familiar with LLM models, but I can look into it and provide some suggestions.\", 'name': 'Michael Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 1\n",
            "\tData: Do you know any alternative LLM models that can match GPT3's capabilities?\n",
            "\tScore: 0.503625393\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"Do you know any alternative LLM models that can match GPT3's capabilities?\", 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 2\n",
            "\tData: GPT3 is a powerful model, no doubt, but its size and computational requirements make it less practical for our semantic search application.\n",
            "\tScore: 0.111373752\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'GPT3 is a powerful model, no doubt, but its size and computational requirements make it less practical for our semantic search application.', 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 3\n",
            "\tData: UniLM is another alternative worth considering. Its competitive performance across various NLP tasks makes it a strong candidate for semantic search in our application.\n",
            "\tScore: 0.104968593\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'UniLM is another alternative worth considering. Its competitive performance across various NLP tasks makes it a strong candidate for semantic search in our application.', 'name': 'Michael Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 4\n",
            "\tData: You're right. BERT's fine-tuning process enables us to adapt the model specifically to our semantic search task and improve its performance.\n",
            "\tScore: 0.103794441\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"You're right. BERT's fine-tuning process enables us to adapt the model specifically to our semantic search task and improve its performance.\", 'name': 'Michael Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Generating config model...\n",
            "Generating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [01/Jul/2023 11:35:50] \"POST /generateResponse HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entry: 0\n",
            "\tData: I heard OpenAI has released GPT4, which is supposed to be an improvement over GPT3.\n",
            "\tScore: 0.617456317\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'I heard OpenAI has released GPT4, which is supposed to be an improvement over GPT3.', 'name': 'Alice Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 1\n",
            "\tData: GPT3 has been the go-to choice for most applications, but exploring alternatives is always a good idea.\n",
            "\tScore: 0.603534818\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'GPT3 has been the go-to choice for most applications, but exploring alternatives is always a good idea.', 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 2\n",
            "\tData: I believe Google's Reformer model could be a potential alternative to GPT3. It's designed to handle long-range dependencies efficiently.\n",
            "\tScore: 0.600437\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"I believe Google's Reformer model could be a potential alternative to GPT3. It's designed to handle long-range dependencies efficiently.\", 'name': 'Michael Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 3\n",
            "\tData: GPT3 has set the bar high, but we should keep an eye on research papers and see if any promising alternatives are being developed.\n",
            "\tScore: 0.591120064\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'GPT3 has set the bar high, but we should keep an eye on research papers and see if any promising alternatives are being developed.', 'name': 'Sarah Davis', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 4\n",
            "\tData: We could consider using GPT-2 as an alternative to GPT3. Although it's an older model, it still performs well in many applications.\n",
            "\tScore: 0.586583436\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"We could consider using GPT-2 as an alternative to GPT3. Although it's an older model, it still performs well in many applications.\", 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Generating config model...\n",
            "Generating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [01/Jul/2023 11:40:45] \"POST /generateResponse HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entry: 0\n",
            "\tData: I heard OpenAI has released GPT4, which is supposed to be an improvement over GPT3.\n",
            "\tScore: 0.566318274\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'I heard OpenAI has released GPT4, which is supposed to be an improvement over GPT3.', 'name': 'Alice Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 1\n",
            "\tData: I believe Google's Reformer model could be a potential alternative to GPT3. It's designed to handle long-range dependencies efficiently.\n",
            "\tScore: 0.561102748\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"I believe Google's Reformer model could be a potential alternative to GPT3. It's designed to handle long-range dependencies efficiently.\", 'name': 'Michael Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 2\n",
            "\tData: Do you know any alternative LLM models that can match GPT3's capabilities?\n",
            "\tScore: 0.551980138\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"Do you know any alternative LLM models that can match GPT3's capabilities?\", 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 3\n",
            "\tData: GPT3 has been the go-to choice for most applications, but exploring alternatives is always a good idea.\n",
            "\tScore: 0.544157684\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'GPT3 has been the go-to choice for most applications, but exploring alternatives is always a good idea.', 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 4\n",
            "\tData: We could consider using GPT-2 as an alternative to GPT3. Although it's an older model, it still performs well in many applications.\n",
            "\tScore: 0.540433168\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"We could consider using GPT-2 as an alternative to GPT3. Although it's an older model, it still performs well in many applications.\", 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Generating config model...\n",
            "Generating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [01/Jul/2023 11:41:16] \"POST /generateResponse HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entry: 0\n",
            "\tData: GPT3 has been the go-to choice for most applications, but exploring alternatives is always a good idea.\n",
            "\tScore: 0.759640455\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'GPT3 has been the go-to choice for most applications, but exploring alternatives is always a good idea.', 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 1\n",
            "\tData: We could consider using GPT-2 as an alternative to GPT3. Although it's an older model, it still performs well in many applications.\n",
            "\tScore: 0.727690578\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"We could consider using GPT-2 as an alternative to GPT3. Although it's an older model, it still performs well in many applications.\", 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 2\n",
            "\tData: GPT3 has set the bar high, but we should keep an eye on research papers and see if any promising alternatives are being developed.\n",
            "\tScore: 0.724249542\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'GPT3 has set the bar high, but we should keep an eye on research papers and see if any promising alternatives are being developed.', 'name': 'Sarah Davis', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 3\n",
            "\tData: Has anyone tried EleutherAI's GPT-Neo? It's an open-source alternative to GPT3 and might be worth exploring.\n",
            "\tScore: 0.7038849\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"Has anyone tried EleutherAI's GPT-Neo? It's an open-source alternative to GPT3 and might be worth exploring.\", 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 4\n",
            "\tData: Do you know any alternative LLM models that can match GPT3's capabilities?\n",
            "\tScore: 0.694109\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"Do you know any alternative LLM models that can match GPT3's capabilities?\", 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Generating config model...\n",
            "Generating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [01/Jul/2023 11:42:03] \"POST /generateResponse HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entry: 0\n",
            "\tData: GPT3 has been the go-to choice for most applications, but exploring alternatives is always a good idea.\n",
            "\tScore: 0.594428\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'GPT3 has been the go-to choice for most applications, but exploring alternatives is always a good idea.', 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 1\n",
            "\tData: Has anyone tried EleutherAI's GPT-Neo? It's an open-source alternative to GPT3 and might be worth exploring.\n",
            "\tScore: 0.592474341\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"Has anyone tried EleutherAI's GPT-Neo? It's an open-source alternative to GPT3 and might be worth exploring.\", 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 2\n",
            "\tData: GPT3 has set the bar high, but we should keep an eye on research papers and see if any promising alternatives are being developed.\n",
            "\tScore: 0.577308476\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'GPT3 has set the bar high, but we should keep an eye on research papers and see if any promising alternatives are being developed.', 'name': 'Sarah Davis', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 3\n",
            "\tData: I heard OpenAI has released GPT4, which is supposed to be an improvement over GPT3.\n",
            "\tScore: 0.553809166\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'I heard OpenAI has released GPT4, which is supposed to be an improvement over GPT3.', 'name': 'Alice Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 4\n",
            "\tData: I've been using GPT2 for similar tasks, and it has been working well for me.\n",
            "\tScore: 0.54715234\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"I've been using GPT2 for similar tasks, and it has been working well for me.\", 'name': 'Sarah Davis', 'time': datetime.date(2023, 6, 19)}\n",
            "Generating config model...\n",
            "Generating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [01/Jul/2023 11:42:40] \"POST /generateResponse HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entry: 0\n",
            "\tData: Has anyone tried EleutherAI's GPT-Neo? It's an open-source alternative to GPT3 and might be worth exploring.\n",
            "\tScore: 0.545931518\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"Has anyone tried EleutherAI's GPT-Neo? It's an open-source alternative to GPT3 and might be worth exploring.\", 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 1\n",
            "\tData: GPT3 has set the bar high, but we should keep an eye on research papers and see if any promising alternatives are being developed.\n",
            "\tScore: 0.538762629\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'GPT3 has set the bar high, but we should keep an eye on research papers and see if any promising alternatives are being developed.', 'name': 'Sarah Davis', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 2\n",
            "\tData: GPT3 has been the go-to choice for most applications, but exploring alternatives is always a good idea.\n",
            "\tScore: 0.537253559\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'GPT3 has been the go-to choice for most applications, but exploring alternatives is always a good idea.', 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 3\n",
            "\tData: I heard OpenAI has released GPT4, which is supposed to be an improvement over GPT3.\n",
            "\tScore: 0.5306\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'I heard OpenAI has released GPT4, which is supposed to be an improvement over GPT3.', 'name': 'Alice Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 4\n",
            "\tData: We could consider using GPT-2 as an alternative to GPT3. Although it's an older model, it still performs well in many applications.\n",
            "\tScore: 0.490214139\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"We could consider using GPT-2 as an alternative to GPT3. Although it's an older model, it still performs well in many applications.\", 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Generating config model...\n",
            "Generating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [01/Jul/2023 11:43:02] \"POST /generateResponse HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entry: 0\n",
            "\tData: Has anyone tried EleutherAI's GPT-Neo? It's an open-source alternative to GPT3 and might be worth exploring.\n",
            "\tScore: 0.539209783\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"Has anyone tried EleutherAI's GPT-Neo? It's an open-source alternative to GPT3 and might be worth exploring.\", 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 1\n",
            "\tData: I heard OpenAI has released GPT4, which is supposed to be an improvement over GPT3.\n",
            "\tScore: 0.513031\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'I heard OpenAI has released GPT4, which is supposed to be an improvement over GPT3.', 'name': 'Alice Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 2\n",
            "\tData: GPT3 has set the bar high, but we should keep an eye on research papers and see if any promising alternatives are being developed.\n",
            "\tScore: 0.485823244\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'GPT3 has set the bar high, but we should keep an eye on research papers and see if any promising alternatives are being developed.', 'name': 'Sarah Davis', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 3\n",
            "\tData: We could consider using GPT-2 as an alternative to GPT3. Although it's an older model, it still performs well in many applications.\n",
            "\tScore: 0.4840644\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"We could consider using GPT-2 as an alternative to GPT3. Although it's an older model, it still performs well in many applications.\", 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 4\n",
            "\tData: I think Microsoft's Turing model could be a potential alternative to GPT3. It's worth checking out.\n",
            "\tScore: 0.481082469\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"I think Microsoft's Turing model could be a potential alternative to GPT3. It's worth checking out.\", 'name': 'Alice Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Generating config model...\n",
            "Generating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [01/Jul/2023 11:43:41] \"POST /generateResponse HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entry: 0\n",
            "\tData: Has anyone tried EleutherAI's GPT-Neo? It's an open-source alternative to GPT3 and might be worth exploring.\n",
            "\tScore: 0.0778465569\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"Has anyone tried EleutherAI's GPT-Neo? It's an open-source alternative to GPT3 and might be worth exploring.\", 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 1\n",
            "\tData: Have you looked into CTRL? It's a conditional transformer language model that allows fine-grained control over the generated text.\n",
            "\tScore: 0.0676919\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"Have you looked into CTRL? It's a conditional transformer language model that allows fine-grained control over the generated text.\", 'name': 'Alice Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 2\n",
            "\tData: I'm not familiar with LLM models, but I can look into it and provide some suggestions.\n",
            "\tScore: 0.0596404299\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"I'm not familiar with LLM models, but I can look into it and provide some suggestions.\", 'name': 'Michael Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 3\n",
            "\tData: I heard OpenAI is working on GPT4, which might offer significant improvements over GPT3. We should keep an eye on their advancements.\n",
            "\tScore: 0.0541050322\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'I heard OpenAI is working on GPT4, which might offer significant improvements over GPT3. We should keep an eye on their advancements.', 'name': 'Sarah Davis', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 4\n",
            "\tData: What about XLNet? It's a transformer-based model that has shown promising results in various NLP benchmarks.\n",
            "\tScore: 0.0505033918\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"What about XLNet? It's a transformer-based model that has shown promising results in various NLP benchmarks.\", 'name': 'Michael Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Generating config model...\n",
            "Generating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [01/Jul/2023 11:44:03] \"POST /generateResponse HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entry: 0\n",
            "\tData: I'm not familiar with LLM models, but I can look into it and provide some suggestions.\n",
            "\tScore: 0.827054739\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"I'm not familiar with LLM models, but I can look into it and provide some suggestions.\", 'name': 'Michael Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 1\n",
            "\tData: Do you know any alternative LLM models that can match GPT3's capabilities?\n",
            "\tScore: 0.657356203\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"Do you know any alternative LLM models that can match GPT3's capabilities?\", 'name': 'John Smith', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 2\n",
            "\tData: What about XLNet? It's a transformer-based model that has shown promising results in various NLP benchmarks.\n",
            "\tScore: 0.277602\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"What about XLNet? It's a transformer-based model that has shown promising results in various NLP benchmarks.\", 'name': 'Michael Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 3\n",
            "\tData: UniLM is another alternative worth considering. Its competitive performance across various NLP tasks makes it a strong candidate for semantic search in our application.\n",
            "\tScore: 0.276117414\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': 'UniLM is another alternative worth considering. Its competitive performance across various NLP tasks makes it a strong candidate for semantic search in our application.', 'name': 'Michael Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Entry: 4\n",
            "\tData: If you're looking for a more lightweight alternative, you can consider DistilBERT. It's a distilled version of BERT that offers faster inference times.\n",
            "\tScore: 0.260996401\n",
            "\tLink: {'channel': 'gptgenerated', 'ispublic': True, 'message': \"If you're looking for a more lightweight alternative, you can consider DistilBERT. It's a distilled version of BERT that offers faster inference times.\", 'name': 'Alice Johnson', 'time': datetime.date(2023, 6, 19)}\n",
            "Generating config model...\n",
            "Generating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [01/Jul/2023 11:44:36] \"POST /generateResponse HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8wI4s6fbzhXc"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c2197a075e664905b8bd007de73dde47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4984914dcb504be7b1ddcd9f313b03c7",
              "IPY_MODEL_019dd42f63ff43668f74e716495c9c93",
              "IPY_MODEL_e983281168dd4990a4c4fcd1d4cca79c"
            ],
            "layout": "IPY_MODEL_e956e4385b7b4b7790e9069c267579aa"
          }
        },
        "4984914dcb504be7b1ddcd9f313b03c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba4f2a7caf1a47b68c75bb22291b2c43",
            "placeholder": "​",
            "style": "IPY_MODEL_21d1cfa65808472291f0cf1a00b3dace",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "019dd42f63ff43668f74e716495c9c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7da55f0eca024550bb9cb57c95524959",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82ab267003084f52933c7df21e6a5542",
            "value": 33
          }
        },
        "e983281168dd4990a4c4fcd1d4cca79c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e1e61da7bdd46f4b3121f79121a72c7",
            "placeholder": "​",
            "style": "IPY_MODEL_4b8001d09a494c8db3ee7ec9c0299b25",
            "value": " 33/33 [01:13&lt;00:00,  2.30s/it]"
          }
        },
        "e956e4385b7b4b7790e9069c267579aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba4f2a7caf1a47b68c75bb22291b2c43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21d1cfa65808472291f0cf1a00b3dace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7da55f0eca024550bb9cb57c95524959": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82ab267003084f52933c7df21e6a5542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e1e61da7bdd46f4b3121f79121a72c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b8001d09a494c8db3ee7ec9c0299b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}